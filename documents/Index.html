<DOCTYPE HTML PUBLIC "-//Netscape Comm. Corp.//DTD HTML//EN">
<HTML>
<HEAD>
<TITLE>Jason Hutchens: documents</TITLE>
<LINK REL="StyleSheet" HREF="style/ai.css" TYPE="text/css" MEDIA="screen">
<LINK REV="Made" HREF="mailto:hutch@tmbg.org">
</HEAD>
<BODY>
<H1><A HREF="../">Jason Hutchens</A>: documents</H1>
<P>This web page contains all of the documents I have written while at
Artificial Intelligence.  From here you can go back to
<A HREF="../">Jason's Ai Home Page</A> for more goodies!</P>
<HR>
<H2>Conference Papers</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>PostScript</TH>
<TH>PDF</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>Conversing with Stochastic Language Models</TD>
<TD>In this paper we look at the problem of conversation simulation via
stochastic
language modelling.  In particular, we show how a stochastic language model
may be used to discover constraints which are beneficial when using the model
generatively in the context of conversation simulation, we introduce a
goal-oriented language model which is conducive to having its generations
constrained in this way, and we discuss how the UpWrite, a technique borrowed
from the field of syntactic pattern recognition, may be used to abstract the
model's representation of user input by discovering higher-level structure
in the observed data.</TD>
<TD><A HREF="cpjh01.ps">cpjh01.ps</A></TD>
<TD><A HREF="cpjh01.pdf">cpjh01.pdf</A></TD>
</TR>
<TR>
<TD>JH02</TD>
<TD>The Developmental Approach to Evaluating Artificial Intelligence-A Proposal</TD>
<TD>We propose developmental evaluation metrics for artificial intelligence
that are based on two assumptions: that the Turing Test provides a sufficient
subjective measure of artificial intelligence, and that any system capable of
passing the Turing Test will necessarily incorporate behavioristic learning
techniques.</TD>
<TD><A HREF="cpjh02.ps">cpjh02.ps</A></TD>
<TD><A HREF="cpjh02.pdf">cpjh02.pdf</A></TD>
</TR>
<TR>
<TD>JH03</TD>
<TD>Creating AI: A Unique Interplay Between the Development of Learning
Algorithms
and their Education</TD>
<TD>Artificial Intelligence NV is a new research and development company
devoted to the creation of Artificial Intelligence (specifically, the
development of computer programs featuring human-like conversational
capabilities).  As such, we believe that our technology will be the first
to pass the Turing Test. The philosophy driving this project is the firm
conviction that returning to Turing's original vision of building a child
machine and then training it to perform is the only way to achieve our
goal. This paper introduces our development cycle, which is based on a
continuous interplay between the development of learning algorithms and
training them to converse, and gives some encouraging examples of the
performance of a system developed in this environment.</TD>
<TD><A HREF="cpjh03.ps">cpjh03.ps</A></TD>
<TD><A HREF="cpjh03.pdf">cpjh03.pdf</A></TD>
</TR>
</TABLE>
<HR>
<H2>Presentations</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>HTML</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>Conversing with Stochastic Language Models</TD>
<TD>
Stochastic language models have a few interesting properties which make them
useful for text generation. We shall
show that such models may be turned into conversation simulators by
constraining this generation process in various
ways.
</TD>
<TD><A HREF="bellagio/">bellagio</A></TD>
</TR>
<TR>
<TD>JH02</TD>
<TD>Looking Inside WebBot</TD>
<TD>
We would like to explore a simple behaviouristic model which can learn to
associate particular actions with verbal commands.  Information theoretical
methods may be used to achieve this.
</TD>
<TD><A HREF="webbot/">webbot</A></TD>
</TR>
<TR>
<TD>JH03</TD>
<TD>Teaching Language to a Computer</TD>
<TD>
In this talk I will take a look at the problem in general, speculating as to
why progress in the field has been so slow.  I will then give you a brief
glimpse at the company I work for before progressing on to describe the way
we're approaching the problem.  I will end by shooting off on a tangent,
describing a few recent thoughts of mine in the hope of getting them into your
brain where they can multiply and hopefully one day return to spawn.
</TD>
<TD><A HREF="sheffield/">sheffield</A></TD>
</TR>
</TABLE>
<HR>
<H2>Technical Reports</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>PostScript</TH>
<TH>PDF</TH>
<TH>RTF</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>Insights into the Problem of Syntactic Category Acquisition</TD>
<TD>A investigation of why various types of ambiguity make the automatic
discovery of syntactic categories difficult, including a look at how the
various ambiguities manifest themselves in the observed data.  Knowledge
of the structure we are likely to see may be used to develop new methods
of finding syntactic categories.  This investigation also shows that all
techniques used to date have been misguided.</TD>
<TD><A HREF="trjh01.ps">trjh01.ps</A></TD>
<TD><A HREF="trjh01.pdf">trjh01.pdf</A></TD>
<TD><A HREF="trjh01.rtf">trjh01.rtf</A></TD>
</TR>
<TR>
<TD>JH02</TD>
<TD>Creating Artificial Intelligence: A Research Philosophy</TD>
<TD>In this paper we present our basic research philosophy.  We begin by
considering
the curve that development of the system will most likely follow.  We then look
at the UpWrite, and go on to speculate as to how the behaviourist approach might
be incorporated.  We end by looking at the various pattern recognition abilities
which the system must have, and by specifying the training methodology.</TD>
<TD><A HREF="trjh02.ps">trjh02.ps</A></TD>
<TD><A HREF="trjh02.pdf">trjh02.pdf</A></TD>
<TD><A HREF="trjh02.rtf">trjh02.rtf</A></TD>
</TR>
<TR>
<TD>JH03</TD>
<TD>Informational Magnetism and WebBot</TD>
<TD>We have recently been exploring the notion of "informational magnetism",
a force of attraction (or repulsion) between two pieces of information,
be they characters, words, sentences or anything else quantifiable.
In this paper we define informational magnetism, and show how a "magnetic
language model" may be used to make predictions about a sequence, and to
generate novel sequences.  We then show how the notion of informational
magnetism is used in WebBot, a simple proof-of-concept web-based natural
language navigation interface.  We conclude with speculations of future work.</TD>
<TD><A HREF="trjh03.ps">trjh03.ps</A></TD>
<TD><A HREF="trjh03.pdf">trjh03.pdf</A></TD>
<TD><A HREF="trjh03.rtf">trjh03.rtf</A></TD>
</TR>
<TR>
<TD>JH04</TD>
<TD>Rock-Paper-Scissors, Arms Races and Language Acquisition</TD>
<TD>Trying to get a computer program to play the game of Rock-Paper-Scissors
is quite difficult, particularly if you want it to win against weak
opponents.  Competing programs mirror arms races found in evolution.  Strong
programs are excellent predictors of their opponent's behaviour.  A similar
version of the game may be devised which mirrors the the natural language
conversation task.</TD>
<TD><A HREF="trjh04.ps">trjh04.ps</A></TD>
<TD><A HREF="trjh04.pdf">trjh04.pdf</A></TD>
<TD><A HREF="trjh04.rtf">trjh04.rtf</A></TD>
</TR>
<TR>
<TD>JH05</TD>
<TD>The Pros and Cons of Various Ways of Looking at the World</TD>
<TD>In the domains of computational language acquisition, understanding and
generation, two viewpoints prevail: the top-down view favoured by the
linguistics community, and the bottom-up view favoured by the engineering
community.  This rather esoteric paper considers the pros and cons of various
ways of looking at the world, showing that the UpWrite embraces both the
top-down and bottom-up viewpoints.</TD>
<TD><A HREF="trjh05.ps">trjh05.ps</A></TD>
<TD><A HREF="trjh05.pdf">trjh05.pdf</A></TD>
<TD><A HREF="trjh05.rtf">trjh05.rtf</A></TD>
</TR>
<TR>
<TD>JH07</TD>
<TD>Existing Technologies Suitable for Version One</TD>
<TD>
The first commercial release will be based on existing technologies, in the
sense that we currently have a pretty good idea of how to implement it (as
opposed to simply using an off-the-shelf solution).  In this document we shall
begin with a preliminary design, which roughly specified the requirements of
the system.  We shall then examine each of these requirements individually,
and show how they may be implemented using existing technology.
</TD>
<TD><A HREF="trjh07.ps">trjh07.ps</A></TD>
<TD><A HREF="trjh07.pdf">trjh07.pdf</A></TD>
<TD><A HREF="trjh07.rtf">trjh07.rtf</A></TD>
</TR>
<TR>
<TD>JH09</TD>
<TD>Portugal Report</TD>
<TD>
From the 11th to the 14th of
September, 2000, I attended the 5th International Colloquium
on Grammatical Inference (ICGI), the 4th Conference on
Computational Natural Language Learning (CoNLL) and the 2nd
Learning Language in Logic Workshop (LLL) in Lisbon, Portugal.  In this
brief report I will attempt to describe my experiences.
</TD>
<TD><A HREF="trjh09.ps">trjh09.ps</A></TD>
<TD><A HREF="trjh09.pdf">trjh09.pdf</A></TD>
<TD><A HREF="trjh09.rtf">trjh09.rtf</A></TD>
</TR>
<TR>
<TD>JH10</TD>
<TD>London Report</TD>
<TD>
From the 6th to the 22nd of October,
2000, I travelled to London for a bit of a rest from work, to see my friends,
and especially my girlfriend, to visit a handful of people whom I believed
would be interested in our work, and to get them involved in some way and in
order to hide out while the Israeli/Palestinian conflict blew over.  This last
point was unanticipated yet fortuitous, as far as my mum's concerned.  I
extended my stay for six days longer than anticipated for this reason alone.
In this brief paper I'll discuss my meetings with Nick Chater, David MacKay,
Martin Redington and Paul Rogers.
</TD>
<TD><A HREF="trjh10.ps">trjh10.ps</A></TD>
<TD><A HREF="trjh10.pdf">trjh10.pdf</A></TD>
<TD><A HREF="trjh10.rtf">trjh10.rtf</A></TD>
</TR>
<TR>
<TD>JH11</TD>
<TD>Simple Beginnings: The Design of HALone</TD>
<TD>
We have set ourselves a difficult task---build a child machine which can
acquire language.  One can be quite taken aback with the enormity of this
venture, and it may be easy to throw our hands up in despair, as simply
nobody knows how to proceed.  The human brain is capable of so much, and
we know so little about it.  Decades of work in linguistics has brought us
no closer to an understanding of how language is acquired or represented
in the brain.  So where do we begin?
</TD>
<TD><A HREF="trjh11.ps">trjh11.ps</A></TD>
<TD><A HREF="trjh11.pdf">trjh11.pdf</A></TD>
<TD><A HREF="trjh11.rtf">trjh11.rtf</A></TD>
</TR>
<TR>
<TD>JH12</TD>
<TD>The Frame Problem</TD>
<TD>
Daniel Dennett has had a lot to say about the frame problem, and
now it's my turn.  A recent issue here at Ai has revolved around showcasing
our technology on our web site via an online conversational interface which
we've chosen to call "Webbot".  The issue has boiled down to deciding
whether the web site should be made aware of Webbot, or whether Webbot should
be another interface layer between the user and the web site.
</TD>
<TD><A HREF="trjh12.ps">trjh12.ps</A></TD>
<TD><A HREF="trjh12.pdf">trjh12.pdf</A></TD>
<TD><A HREF="trjh12.rtf">trjh12.rtf</A></TD>
</TR>
<TR>
<TD>JH17</TD>
<TD>HALtwo Brain Design</TD>
<TD>The birth of HALone was considerably more painful than
expected, and this was largely due to what we call \E{the representation
problem}.  We have recently had some insights which make this problem
moot.  Looking at things from a different angle make the problem
disappear.  Unfortunately the damage has already been done---the
implementation of HALone is messier than need be due to the hacking which
went on in the week or two prior to its delivery.</TD>
<TD><A HREF="trjh17.ps">trjh17.ps</A></TD>
<TD><A HREF="trjh17.pdf">trjh17.pdf</A></TD>
<TD><A HREF="trjh17.rtf">trjh17.rtf</A></TD>
</TR>
<TR>
<TD>JH19</TD>
<TD>Infomagnetism and Sentence Generation</TD>
<TD>
Lexical attraction was introduced by Doug Beeferman, Adam
Berger
and John Lafferty in a 1997 conference paper, and was
picked
up by Deniz Yuret in his 1998 thesis which focused on using lexical
attraction
to discover a link-grammar for English utterances.
Searching
for "lexical attraction" on Google reveals links to these papers, and
	more.</TD>
<TD><A HREF="trjh19.ps">trjh19.ps</A></TD>
<TD><A HREF="trjh19.pdf">trjh19.pdf</A></TD>
<TD><A HREF="trjh19.rtf">trjh19.rtf</A></TD>
</TR>
<TR>
<TD>JH21</TD>
<TD>Infomagnetism Revisited</TD>
<TD>
Yaki recently made some remarks following the release of my
previous technical report on infomagnetism (TRJH19).  In this brief paper
I will attempt to address the points he raised.
</TD>
<TD><A HREF="trjh21.ps">trjh21.ps</A></TD>
<TD><A HREF="trjh21.pdf">trjh21.pdf</A></TD>
<TD><A HREF="trjh21.rtf">trjh21.rtf</A></TD>
</TR>
<TR>
<TD>JH23</TD>
<TD>The Chinese Room Experiment</TD>
<TD>
John Searle, the philosopher, formulated the Chinese Room
thought experiment as an argument against strong AI.  Roughly, the
experiment replaces the CPU in a computer with a human being, and shows
that even if a computer program were written which could hold a meaningful
conversation in Chinese we couldn't conclude that the human being inside
the computer, who would only be manipulating symbols to make the program
run, would truly "understand" Chinese as a result.
</TD>
<TD><A HREF="trjh23.ps">trjh23.ps</A></TD>
<TD><A HREF="trjh23.pdf">trjh23.pdf</A></TD>
<TD><A HREF="trjh23.rtf">trjh23.rtf</A></TD>
</TR>
<TR>
<TD>JH26</TD>
<TD>Learning by Rote with Multiple Alignment</TD>
<TD>
The current version of HAL assumes an alphabet of characters.
This default representation is obviously insufficient, so HAL is given the
ability to add new symbols to its alphabet dynamically.  Currently it does
this by hypothesising chunks---symbols formed by joining two existing
symbols together, and adding these chunks to the alphabet whenever doing
so would result in improved performance.
</TD>
<TD><A HREF="trjh26.ps">trjh26.ps</A></TD>
<TD><A HREF="trjh26.pdf">trjh26.pdf</A></TD>
<TD><A HREF="trjh26.rtf">trjh26.rtf</A></TD>
</TR>
</TABLE>
<HR>
<H2>Design Documentation</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>PostScript</TH>
<TH>PDF</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>WebBot</TD>
<TD>Complete documentation of the WebBot project, including a full listing of
the source code.</TD>
<TD><A HREF="ddjh01.ps">ddjh01.ps</A></TD>
<TD><A HREF="ddjh01.pdf">ddjh01.pdf</A></TD>
</TR>
</TABLE>
<HR>
<H2>Meeting Notes</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>PostScript</TH>
<TH>PDF</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>Notes From Mike Session #1</TD>
<TD>This short report describes a meeting between Mike, Jason, Yaki and Yuval
held on the morning of Wednesday the 26th of July, 2000.  It's
divided into rough sections, with "action points" listed in the conclusion.
These sections are given in logical, not chronological, order.  The sections
cover the timeline of the research track, the question of training, and
administrative and technical issues.</TD>
<TD><A HREF="mmjh01.ps">mmjh01.ps</A></TD>
<TD><A HREF="mmjh01.pdf">mmjh01.pdf</A></TD>
</TR>
</TABLE>
<HR>
<H2>Computer Generated</H2>
<TABLE WIDTH="100%">
<TR>
<TH>Number</TH>
<TH>Title</TH>
<TH>Description</TH>
<TH>PostScript</TH>
<TH>PDF</TH>
</TR>
<TR>
<TD>JH01</TD>
<TD>Dreams and Electric Dreams</TD>
<TD>
This document contains two short stories.  The first, "Dreams", was written
over one hundred years ago by Jerome Klapta Jerome, an English author of comic
stories famous for his "Three Men in a Boat".  The second story, "Electric
Dreams", was written by a computer program, running on my Dell Latitude
laptop.
</TD>
<TD><A HREF="cgjh01.ps">cgjh01.ps</A></TD>
<TD><A HREF="cgjh01.pdf">cgjh01.pdf</A></TD>
</TR>
</TABLE>
</BODY>
</HTML>
